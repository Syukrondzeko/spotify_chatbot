{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0131e39-9d1f-4be1-9dc9-b9ac08340a81",
   "metadata": {},
   "source": [
    "# 1. Vector Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa4d91d5-a43b-451c-935f-ee21c5a12748",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 15:53:36,580 - INFO - Loading dataset\n",
      "2024-10-30 15:54:10,070 - INFO - Dataset loaded in 33.49 seconds\n",
      "2024-10-30 15:54:10,081 - INFO - Starting text preprocessing\n",
      "2024-10-30 15:54:54,965 - INFO - Text preprocessing completed in 44.88 seconds\n",
      "2024-10-30 15:54:55,800 - INFO - Use pytorch device_name: mps\n",
      "2024-10-30 15:54:55,801 - INFO - Load pretrained SentenceTransformer: /Users/muhamadsyukron/Main Folder/Mac 2023 Files/Mekari/chatbot_project/models/all-MiniLM-L6-v2\n",
      "2024-10-30 15:54:57,020 - INFO - Starting embedding\n",
      "2024-10-30 15:54:57,032 - INFO - Total rows to embed: 1250338\n",
      "2024-10-30 15:55:27,768 - INFO - Saved 1000 embeddings to embeddings_batch_1.json\n",
      "2024-10-30 15:55:53,099 - INFO - Saved 2000 embeddings to embeddings_batch_2.json\n",
      "2024-10-30 15:56:18,641 - INFO - Saved 3000 embeddings to embeddings_batch_3.json\n",
      "2024-10-30 15:56:44,155 - INFO - Saved 4000 embeddings to embeddings_batch_4.json\n",
      "2024-10-30 15:57:09,490 - INFO - Saved 5000 embeddings to embeddings_batch_5.json\n",
      "2024-10-30 15:57:34,752 - INFO - Saved 6000 embeddings to embeddings_batch_6.json\n",
      "2024-10-30 15:57:59,908 - INFO - Saved 7000 embeddings to embeddings_batch_7.json\n",
      "2024-10-30 15:58:25,004 - INFO - Saved 8000 embeddings to embeddings_batch_8.json\n",
      "2024-10-30 15:58:50,190 - INFO - Saved 9000 embeddings to embeddings_batch_9.json\n",
      "2024-10-30 15:59:15,722 - INFO - Saved 10000 embeddings to embeddings_batch_10.json\n",
      "2024-10-30 15:59:41,039 - INFO - Saved 11000 embeddings to embeddings_batch_11.json\n",
      "2024-10-30 16:00:06,241 - INFO - Saved 12000 embeddings to embeddings_batch_12.json\n",
      "2024-10-30 16:00:31,386 - INFO - Saved 13000 embeddings to embeddings_batch_13.json\n",
      "2024-10-30 16:00:56,567 - INFO - Saved 14000 embeddings to embeddings_batch_14.json\n",
      "2024-10-30 16:01:23,178 - INFO - Saved 15000 embeddings to embeddings_batch_15.json\n",
      "2024-10-30 16:01:48,367 - INFO - Saved 16000 embeddings to embeddings_batch_16.json\n",
      "2024-10-30 16:02:13,831 - INFO - Saved 17000 embeddings to embeddings_batch_17.json\n",
      "2024-10-30 16:02:38,902 - INFO - Saved 18000 embeddings to embeddings_batch_18.json\n",
      "2024-10-30 16:03:04,317 - INFO - Saved 19000 embeddings to embeddings_batch_19.json\n",
      "2024-10-30 16:03:29,529 - INFO - Saved 20000 embeddings to embeddings_batch_20.json\n",
      "2024-10-30 16:03:54,958 - INFO - Saved 21000 embeddings to embeddings_batch_21.json\n",
      "2024-10-30 16:04:20,522 - INFO - Saved 22000 embeddings to embeddings_batch_22.json\n",
      "2024-10-30 16:04:45,794 - INFO - Saved 23000 embeddings to embeddings_batch_23.json\n",
      "2024-10-30 16:05:11,364 - INFO - Saved 24000 embeddings to embeddings_batch_24.json\n",
      "2024-10-30 16:05:36,555 - INFO - Saved 25000 embeddings to embeddings_batch_25.json\n",
      "2024-10-30 16:06:02,730 - INFO - Saved 26000 embeddings to embeddings_batch_26.json\n",
      "2024-10-30 16:06:28,070 - INFO - Saved 27000 embeddings to embeddings_batch_27.json\n",
      "2024-10-30 16:06:53,550 - INFO - Saved 28000 embeddings to embeddings_batch_28.json\n",
      "2024-10-30 16:07:19,988 - INFO - Saved 29000 embeddings to embeddings_batch_29.json\n",
      "2024-10-30 16:07:49,316 - INFO - Saved 30000 embeddings to embeddings_batch_30.json\n",
      "2024-10-30 16:08:26,065 - INFO - Saved 31000 embeddings to embeddings_batch_31.json\n",
      "2024-10-30 16:08:57,257 - INFO - Saved 32000 embeddings to embeddings_batch_32.json\n",
      "2024-10-30 16:09:24,429 - INFO - Saved 33000 embeddings to embeddings_batch_33.json\n",
      "2024-10-30 16:09:54,034 - INFO - Saved 34000 embeddings to embeddings_batch_34.json\n",
      "2024-10-30 16:10:21,558 - INFO - Saved 35000 embeddings to embeddings_batch_35.json\n",
      "2024-10-30 16:10:46,707 - INFO - Saved 36000 embeddings to embeddings_batch_36.json\n",
      "2024-10-30 16:11:11,794 - INFO - Saved 37000 embeddings to embeddings_batch_37.json\n",
      "2024-10-30 16:11:37,423 - INFO - Saved 38000 embeddings to embeddings_batch_38.json\n",
      "2024-10-30 16:12:02,551 - INFO - Saved 39000 embeddings to embeddings_batch_39.json\n",
      "2024-10-30 16:12:27,948 - INFO - Saved 40000 embeddings to embeddings_batch_40.json\n",
      "2024-10-30 16:12:53,393 - INFO - Saved 41000 embeddings to embeddings_batch_41.json\n",
      "2024-10-30 16:13:19,043 - INFO - Saved 42000 embeddings to embeddings_batch_42.json\n",
      "2024-10-30 16:13:44,622 - INFO - Saved 43000 embeddings to embeddings_batch_43.json\n",
      "2024-10-30 16:14:12,021 - INFO - Saved 44000 embeddings to embeddings_batch_44.json\n",
      "2024-10-30 16:14:39,078 - INFO - Saved 45000 embeddings to embeddings_batch_45.json\n",
      "2024-10-30 16:15:05,690 - INFO - Saved 46000 embeddings to embeddings_batch_46.json\n",
      "2024-10-30 16:15:31,548 - INFO - Saved 47000 embeddings to embeddings_batch_47.json\n",
      "2024-10-30 16:15:56,732 - INFO - Saved 48000 embeddings to embeddings_batch_48.json\n",
      "2024-10-30 16:16:22,440 - INFO - Saved 49000 embeddings to embeddings_batch_49.json\n",
      "2024-10-30 16:16:48,213 - INFO - Saved 50000 embeddings to embeddings_batch_50.json\n",
      "2024-10-30 16:17:13,565 - INFO - Saved 51000 embeddings to embeddings_batch_51.json\n",
      "2024-10-30 16:17:39,719 - INFO - Saved 52000 embeddings to embeddings_batch_52.json\n",
      "2024-10-30 16:18:05,084 - INFO - Saved 53000 embeddings to embeddings_batch_53.json\n",
      "2024-10-30 16:18:31,075 - INFO - Saved 54000 embeddings to embeddings_batch_54.json\n",
      "2024-10-30 16:18:57,072 - INFO - Saved 55000 embeddings to embeddings_batch_55.json\n",
      "2024-10-30 16:19:22,473 - INFO - Saved 56000 embeddings to embeddings_batch_56.json\n",
      "2024-10-30 16:19:49,890 - INFO - Saved 57000 embeddings to embeddings_batch_57.json\n",
      "2024-10-30 16:20:15,474 - INFO - Saved 58000 embeddings to embeddings_batch_58.json\n",
      "2024-10-30 16:20:41,323 - INFO - Saved 59000 embeddings to embeddings_batch_59.json\n",
      "2024-10-30 16:21:06,830 - INFO - Saved 60000 embeddings to embeddings_batch_60.json\n",
      "2024-10-30 16:21:33,467 - INFO - Saved 61000 embeddings to embeddings_batch_61.json\n",
      "2024-10-30 16:22:03,154 - INFO - Saved 62000 embeddings to embeddings_batch_62.json\n",
      "2024-10-30 16:22:29,355 - INFO - Saved 63000 embeddings to embeddings_batch_63.json\n",
      "2024-10-30 16:22:54,530 - INFO - Saved 64000 embeddings to embeddings_batch_64.json\n",
      "2024-10-30 16:23:19,802 - INFO - Saved 65000 embeddings to embeddings_batch_65.json\n",
      "2024-10-30 16:23:45,808 - INFO - Saved 66000 embeddings to embeddings_batch_66.json\n",
      "2024-10-30 16:24:10,924 - INFO - Saved 67000 embeddings to embeddings_batch_67.json\n",
      "2024-10-30 16:24:35,980 - INFO - Saved 68000 embeddings to embeddings_batch_68.json\n",
      "2024-10-30 16:25:03,015 - INFO - Saved 69000 embeddings to embeddings_batch_69.json\n",
      "2024-10-30 16:25:29,737 - INFO - Saved 70000 embeddings to embeddings_batch_70.json\n",
      "2024-10-30 16:25:54,885 - INFO - Saved 71000 embeddings to embeddings_batch_71.json\n",
      "2024-10-30 16:26:20,024 - INFO - Saved 72000 embeddings to embeddings_batch_72.json\n",
      "2024-10-30 16:26:45,889 - INFO - Saved 73000 embeddings to embeddings_batch_73.json\n",
      "2024-10-30 16:27:12,389 - INFO - Saved 74000 embeddings to embeddings_batch_74.json\n",
      "2024-10-30 16:27:37,673 - INFO - Saved 75000 embeddings to embeddings_batch_75.json\n",
      "2024-10-30 16:28:02,497 - INFO - Saved 76000 embeddings to embeddings_batch_76.json\n",
      "2024-10-30 16:28:27,236 - INFO - Saved 77000 embeddings to embeddings_batch_77.json\n",
      "2024-10-30 16:28:52,448 - INFO - Saved 78000 embeddings to embeddings_batch_78.json\n",
      "2024-10-30 16:29:17,971 - INFO - Saved 79000 embeddings to embeddings_batch_79.json\n",
      "2024-10-30 16:29:43,422 - INFO - Saved 80000 embeddings to embeddings_batch_80.json\n",
      "2024-10-30 16:30:08,466 - INFO - Saved 81000 embeddings to embeddings_batch_81.json\n",
      "2024-10-30 16:30:33,539 - INFO - Saved 82000 embeddings to embeddings_batch_82.json\n",
      "2024-10-30 16:31:00,241 - INFO - Saved 83000 embeddings to embeddings_batch_83.json\n",
      "2024-10-30 16:31:28,831 - INFO - Saved 84000 embeddings to embeddings_batch_84.json\n",
      "2024-10-30 16:31:53,771 - INFO - Saved 85000 embeddings to embeddings_batch_85.json\n",
      "2024-10-30 16:32:18,378 - INFO - Saved 86000 embeddings to embeddings_batch_86.json\n",
      "2024-10-30 16:32:43,389 - INFO - Saved 87000 embeddings to embeddings_batch_87.json\n",
      "2024-10-30 16:33:08,736 - INFO - Saved 88000 embeddings to embeddings_batch_88.json\n",
      "2024-10-30 16:33:33,821 - INFO - Saved 89000 embeddings to embeddings_batch_89.json\n",
      "2024-10-30 16:33:58,316 - INFO - Saved 90000 embeddings to embeddings_batch_90.json\n",
      "2024-10-30 16:34:23,018 - INFO - Saved 91000 embeddings to embeddings_batch_91.json\n",
      "2024-10-30 16:34:47,481 - INFO - Saved 92000 embeddings to embeddings_batch_92.json\n",
      "2024-10-30 16:35:12,054 - INFO - Saved 93000 embeddings to embeddings_batch_93.json\n",
      "2024-10-30 16:35:36,893 - INFO - Saved 94000 embeddings to embeddings_batch_94.json\n",
      "2024-10-30 16:36:03,363 - INFO - Saved 95000 embeddings to embeddings_batch_95.json\n",
      "2024-10-30 16:36:29,945 - INFO - Saved 96000 embeddings to embeddings_batch_96.json\n",
      "2024-10-30 16:36:55,819 - INFO - Saved 97000 embeddings to embeddings_batch_97.json\n",
      "2024-10-30 16:37:22,639 - INFO - Saved 98000 embeddings to embeddings_batch_98.json\n",
      "2024-10-30 16:37:50,799 - INFO - Saved 99000 embeddings to embeddings_batch_99.json\n",
      "2024-10-30 16:38:17,471 - INFO - Saved 100000 embeddings to embeddings_batch_100.json\n",
      "2024-10-30 16:38:43,493 - INFO - Saved 101000 embeddings to embeddings_batch_101.json\n",
      "2024-10-30 16:39:09,922 - INFO - Saved 102000 embeddings to embeddings_batch_102.json\n",
      "2024-10-30 16:39:37,249 - INFO - Saved 103000 embeddings to embeddings_batch_103.json\n",
      "2024-10-30 16:40:07,784 - INFO - Saved 104000 embeddings to embeddings_batch_104.json\n",
      "2024-10-30 16:40:34,627 - INFO - Saved 105000 embeddings to embeddings_batch_105.json\n",
      "2024-10-30 16:41:00,857 - INFO - Saved 106000 embeddings to embeddings_batch_106.json\n",
      "2024-10-30 16:41:27,313 - INFO - Saved 107000 embeddings to embeddings_batch_107.json\n",
      "2024-10-30 16:41:53,927 - INFO - Saved 108000 embeddings to embeddings_batch_108.json\n",
      "2024-10-30 16:42:23,243 - INFO - Saved 109000 embeddings to embeddings_batch_109.json\n",
      "2024-10-30 16:42:49,879 - INFO - Saved 110000 embeddings to embeddings_batch_110.json\n",
      "2024-10-30 16:43:17,751 - INFO - Saved 111000 embeddings to embeddings_batch_111.json\n",
      "2024-10-30 16:43:44,785 - INFO - Saved 112000 embeddings to embeddings_batch_112.json\n",
      "2024-10-30 16:44:12,297 - INFO - Saved 113000 embeddings to embeddings_batch_113.json\n",
      "2024-10-30 16:44:40,520 - INFO - Saved 114000 embeddings to embeddings_batch_114.json\n",
      "2024-10-30 16:45:06,610 - INFO - Saved 115000 embeddings to embeddings_batch_115.json\n",
      "2024-10-30 16:45:33,433 - INFO - Saved 116000 embeddings to embeddings_batch_116.json\n",
      "2024-10-30 16:45:59,628 - INFO - Saved 117000 embeddings to embeddings_batch_117.json\n",
      "2024-10-30 16:46:25,449 - INFO - Saved 118000 embeddings to embeddings_batch_118.json\n",
      "2024-10-30 16:46:51,208 - INFO - Saved 119000 embeddings to embeddings_batch_119.json\n",
      "2024-10-30 16:47:18,251 - INFO - Saved 120000 embeddings to embeddings_batch_120.json\n",
      "2024-10-30 16:47:45,386 - INFO - Saved 121000 embeddings to embeddings_batch_121.json\n",
      "2024-10-30 16:48:11,349 - INFO - Saved 122000 embeddings to embeddings_batch_122.json\n",
      "2024-10-30 16:48:38,575 - INFO - Saved 123000 embeddings to embeddings_batch_123.json\n",
      "2024-10-30 16:49:06,207 - INFO - Saved 124000 embeddings to embeddings_batch_124.json\n",
      "2024-10-30 16:49:33,203 - INFO - Saved 125000 embeddings to embeddings_batch_125.json\n",
      "2024-10-30 16:49:59,187 - INFO - Saved 126000 embeddings to embeddings_batch_126.json\n",
      "2024-10-30 16:50:28,701 - INFO - Saved 127000 embeddings to embeddings_batch_127.json\n",
      "2024-10-30 16:50:55,518 - INFO - Saved 128000 embeddings to embeddings_batch_128.json\n",
      "2024-10-30 16:51:22,544 - INFO - Saved 129000 embeddings to embeddings_batch_129.json\n",
      "2024-10-30 16:51:49,349 - INFO - Saved 130000 embeddings to embeddings_batch_130.json\n",
      "2024-10-30 16:52:16,532 - INFO - Saved 131000 embeddings to embeddings_batch_131.json\n",
      "2024-10-30 16:52:43,493 - INFO - Saved 132000 embeddings to embeddings_batch_132.json\n",
      "2024-10-30 16:53:10,117 - INFO - Saved 133000 embeddings to embeddings_batch_133.json\n",
      "2024-10-30 16:53:36,784 - INFO - Saved 134000 embeddings to embeddings_batch_134.json\n",
      "2024-10-30 16:54:04,340 - INFO - Saved 135000 embeddings to embeddings_batch_135.json\n",
      "2024-10-30 16:54:30,128 - INFO - Saved 136000 embeddings to embeddings_batch_136.json\n",
      "2024-10-30 16:54:59,623 - INFO - Saved 137000 embeddings to embeddings_batch_137.json\n",
      "2024-10-30 16:55:27,489 - INFO - Saved 138000 embeddings to embeddings_batch_138.json\n",
      "2024-10-30 16:55:56,880 - INFO - Saved 139000 embeddings to embeddings_batch_139.json\n",
      "2024-10-30 16:56:24,103 - INFO - Saved 140000 embeddings to embeddings_batch_140.json\n",
      "2024-10-30 16:56:52,671 - INFO - Saved 141000 embeddings to embeddings_batch_141.json\n",
      "2024-10-30 16:57:19,765 - INFO - Saved 142000 embeddings to embeddings_batch_142.json\n",
      "2024-10-30 16:57:49,593 - INFO - Saved 143000 embeddings to embeddings_batch_143.json\n",
      "2024-10-30 16:58:17,446 - INFO - Saved 144000 embeddings to embeddings_batch_144.json\n",
      "2024-10-30 16:58:46,372 - INFO - Saved 145000 embeddings to embeddings_batch_145.json\n",
      "2024-10-30 16:59:13,090 - INFO - Saved 146000 embeddings to embeddings_batch_146.json\n",
      "2024-10-30 16:59:40,625 - INFO - Saved 147000 embeddings to embeddings_batch_147.json\n",
      "2024-10-30 17:00:09,597 - INFO - Saved 148000 embeddings to embeddings_batch_148.json\n",
      "2024-10-30 17:00:36,937 - INFO - Saved 149000 embeddings to embeddings_batch_149.json\n",
      "2024-10-30 17:01:03,988 - INFO - Saved 150000 embeddings to embeddings_batch_150.json\n",
      "2024-10-30 17:01:29,900 - INFO - Saved 151000 embeddings to embeddings_batch_151.json\n",
      "2024-10-30 17:01:56,153 - INFO - Saved 152000 embeddings to embeddings_batch_152.json\n",
      "2024-10-30 17:02:21,693 - INFO - Saved 153000 embeddings to embeddings_batch_153.json\n",
      "2024-10-30 17:02:47,943 - INFO - Saved 154000 embeddings to embeddings_batch_154.json\n",
      "2024-10-30 17:03:14,095 - INFO - Saved 155000 embeddings to embeddings_batch_155.json\n",
      "2024-10-30 17:03:40,589 - INFO - Saved 156000 embeddings to embeddings_batch_156.json\n",
      "2024-10-30 17:04:08,151 - INFO - Saved 157000 embeddings to embeddings_batch_157.json\n",
      "2024-10-30 17:04:35,979 - INFO - Saved 158000 embeddings to embeddings_batch_158.json\n",
      "2024-10-30 17:05:03,070 - INFO - Saved 159000 embeddings to embeddings_batch_159.json\n",
      "2024-10-30 17:05:32,122 - INFO - Saved 160000 embeddings to embeddings_batch_160.json\n",
      "2024-10-30 17:06:04,006 - INFO - Saved 161000 embeddings to embeddings_batch_161.json\n",
      "2024-10-30 17:06:30,211 - INFO - Saved 162000 embeddings to embeddings_batch_162.json\n",
      "2024-10-30 17:06:55,266 - INFO - Saved 163000 embeddings to embeddings_batch_163.json\n",
      "2024-10-30 17:07:21,478 - INFO - Saved 164000 embeddings to embeddings_batch_164.json\n",
      "2024-10-30 17:07:48,362 - INFO - Saved 165000 embeddings to embeddings_batch_165.json\n",
      "2024-10-30 17:08:15,076 - INFO - Saved 166000 embeddings to embeddings_batch_166.json\n",
      "2024-10-30 17:08:42,383 - INFO - Saved 167000 embeddings to embeddings_batch_167.json\n",
      "2024-10-30 17:09:09,132 - INFO - Saved 168000 embeddings to embeddings_batch_168.json\n",
      "2024-10-30 17:09:35,716 - INFO - Saved 169000 embeddings to embeddings_batch_169.json\n",
      "2024-10-30 17:10:01,297 - INFO - Saved 170000 embeddings to embeddings_batch_170.json\n",
      "2024-10-30 17:10:27,593 - INFO - Saved 171000 embeddings to embeddings_batch_171.json\n",
      "2024-10-30 17:10:54,139 - INFO - Saved 172000 embeddings to embeddings_batch_172.json\n",
      "2024-10-30 17:11:20,251 - INFO - Saved 173000 embeddings to embeddings_batch_173.json\n",
      "2024-10-30 17:11:46,089 - INFO - Saved 174000 embeddings to embeddings_batch_174.json\n",
      "2024-10-30 17:12:12,890 - INFO - Saved 175000 embeddings to embeddings_batch_175.json\n",
      "2024-10-30 17:12:39,187 - INFO - Saved 176000 embeddings to embeddings_batch_176.json\n",
      "2024-10-30 17:13:05,891 - INFO - Saved 177000 embeddings to embeddings_batch_177.json\n",
      "2024-10-30 17:13:32,489 - INFO - Saved 178000 embeddings to embeddings_batch_178.json\n",
      "2024-10-30 17:13:58,853 - INFO - Saved 179000 embeddings to embeddings_batch_179.json\n",
      "2024-10-30 17:14:25,593 - INFO - Saved 180000 embeddings to embeddings_batch_180.json\n",
      "2024-10-30 17:14:52,706 - INFO - Saved 181000 embeddings to embeddings_batch_181.json\n",
      "2024-10-30 17:15:18,939 - INFO - Saved 182000 embeddings to embeddings_batch_182.json\n",
      "2024-10-30 17:15:45,995 - INFO - Saved 183000 embeddings to embeddings_batch_183.json\n",
      "2024-10-30 17:16:13,906 - INFO - Saved 184000 embeddings to embeddings_batch_184.json\n",
      "2024-10-30 17:16:40,038 - INFO - Saved 185000 embeddings to embeddings_batch_185.json\n",
      "2024-10-30 17:17:07,221 - INFO - Saved 186000 embeddings to embeddings_batch_186.json\n",
      "2024-10-30 17:17:35,467 - INFO - Saved 187000 embeddings to embeddings_batch_187.json\n",
      "2024-10-30 17:18:01,667 - INFO - Saved 188000 embeddings to embeddings_batch_188.json\n",
      "2024-10-30 17:18:27,484 - INFO - Saved 189000 embeddings to embeddings_batch_189.json\n",
      "2024-10-30 17:18:53,775 - INFO - Saved 190000 embeddings to embeddings_batch_190.json\n",
      "2024-10-30 17:19:19,580 - INFO - Saved 191000 embeddings to embeddings_batch_191.json\n",
      "2024-10-30 17:19:47,298 - INFO - Saved 192000 embeddings to embeddings_batch_192.json\n",
      "2024-10-30 17:20:16,290 - INFO - Saved 193000 embeddings to embeddings_batch_193.json\n",
      "2024-10-30 17:20:42,347 - INFO - Saved 194000 embeddings to embeddings_batch_194.json\n",
      "2024-10-30 17:21:08,320 - INFO - Saved 195000 embeddings to embeddings_batch_195.json\n",
      "2024-10-30 17:21:35,270 - INFO - Saved 196000 embeddings to embeddings_batch_196.json\n",
      "2024-10-30 17:22:01,304 - INFO - Saved 197000 embeddings to embeddings_batch_197.json\n",
      "2024-10-30 17:22:29,202 - INFO - Saved 198000 embeddings to embeddings_batch_198.json\n",
      "2024-10-30 17:23:02,832 - INFO - Saved 199000 embeddings to embeddings_batch_199.json\n",
      "2024-10-30 17:23:31,473 - INFO - Saved 200000 embeddings to embeddings_batch_200.json\n",
      "2024-10-30 17:23:58,128 - INFO - Saved 201000 embeddings to embeddings_batch_201.json\n",
      "2024-10-30 17:24:23,789 - INFO - Saved 202000 embeddings to embeddings_batch_202.json\n",
      "2024-10-30 17:24:49,520 - INFO - Saved 203000 embeddings to embeddings_batch_203.json\n",
      "2024-10-30 17:25:16,607 - INFO - Saved 204000 embeddings to embeddings_batch_204.json\n",
      "2024-10-30 17:25:44,108 - INFO - Saved 205000 embeddings to embeddings_batch_205.json\n",
      "2024-10-30 17:26:13,271 - INFO - Saved 206000 embeddings to embeddings_batch_206.json\n",
      "2024-10-30 17:26:47,351 - INFO - Saved 207000 embeddings to embeddings_batch_207.json\n",
      "2024-10-30 17:27:14,178 - INFO - Saved 208000 embeddings to embeddings_batch_208.json\n",
      "2024-10-30 17:27:39,784 - INFO - Saved 209000 embeddings to embeddings_batch_209.json\n",
      "2024-10-30 17:28:10,925 - INFO - Saved 210000 embeddings to embeddings_batch_210.json\n",
      "2024-10-30 17:28:37,988 - INFO - Saved 211000 embeddings to embeddings_batch_211.json\n",
      "2024-10-30 17:29:05,154 - INFO - Saved 212000 embeddings to embeddings_batch_212.json\n",
      "2024-10-30 17:29:31,913 - INFO - Saved 213000 embeddings to embeddings_batch_213.json\n",
      "2024-10-30 17:29:58,568 - INFO - Saved 214000 embeddings to embeddings_batch_214.json\n",
      "2024-10-30 17:30:26,148 - INFO - Saved 215000 embeddings to embeddings_batch_215.json\n",
      "2024-10-30 17:30:54,584 - INFO - Saved 216000 embeddings to embeddings_batch_216.json\n",
      "2024-10-30 17:31:22,854 - INFO - Saved 217000 embeddings to embeddings_batch_217.json\n",
      "2024-10-30 17:31:52,031 - INFO - Saved 218000 embeddings to embeddings_batch_218.json\n",
      "2024-10-30 17:32:20,787 - INFO - Saved 219000 embeddings to embeddings_batch_219.json\n",
      "2024-10-30 17:32:46,955 - INFO - Saved 220000 embeddings to embeddings_batch_220.json\n",
      "2024-10-30 17:33:12,965 - INFO - Saved 221000 embeddings to embeddings_batch_221.json\n",
      "2024-10-30 17:33:41,899 - INFO - Saved 222000 embeddings to embeddings_batch_222.json\n",
      "2024-10-30 17:34:08,975 - INFO - Saved 223000 embeddings to embeddings_batch_223.json\n",
      "2024-10-30 17:34:37,786 - INFO - Saved 224000 embeddings to embeddings_batch_224.json\n",
      "2024-10-30 17:35:05,240 - INFO - Saved 225000 embeddings to embeddings_batch_225.json\n",
      "2024-10-30 17:35:32,194 - INFO - Saved 226000 embeddings to embeddings_batch_226.json\n",
      "2024-10-30 17:35:58,592 - INFO - Saved 227000 embeddings to embeddings_batch_227.json\n",
      "2024-10-30 17:36:25,605 - INFO - Saved 228000 embeddings to embeddings_batch_228.json\n",
      "2024-10-30 17:36:53,337 - INFO - Saved 229000 embeddings to embeddings_batch_229.json\n",
      "2024-10-30 17:37:19,711 - INFO - Saved 230000 embeddings to embeddings_batch_230.json\n",
      "2024-10-30 17:37:45,715 - INFO - Saved 231000 embeddings to embeddings_batch_231.json\n",
      "2024-10-30 17:38:11,803 - INFO - Saved 232000 embeddings to embeddings_batch_232.json\n",
      "2024-10-30 17:38:38,549 - INFO - Saved 233000 embeddings to embeddings_batch_233.json\n",
      "2024-10-30 17:39:05,213 - INFO - Saved 234000 embeddings to embeddings_batch_234.json\n",
      "2024-10-30 17:39:31,451 - INFO - Saved 235000 embeddings to embeddings_batch_235.json\n",
      "2024-10-30 17:39:57,772 - INFO - Saved 236000 embeddings to embeddings_batch_236.json\n",
      "2024-10-30 17:40:28,990 - INFO - Saved 237000 embeddings to embeddings_batch_237.json\n",
      "2024-10-30 17:40:56,845 - INFO - Saved 238000 embeddings to embeddings_batch_238.json\n",
      "2024-10-30 17:41:25,803 - INFO - Saved 239000 embeddings to embeddings_batch_239.json\n",
      "2024-10-30 17:41:53,382 - INFO - Saved 240000 embeddings to embeddings_batch_240.json\n",
      "2024-10-30 17:42:20,132 - INFO - Saved 241000 embeddings to embeddings_batch_241.json\n",
      "2024-10-30 17:42:48,224 - INFO - Saved 242000 embeddings to embeddings_batch_242.json\n",
      "2024-10-30 17:43:17,750 - INFO - Saved 243000 embeddings to embeddings_batch_243.json\n",
      "2024-10-30 17:43:45,533 - INFO - Saved 244000 embeddings to embeddings_batch_244.json\n",
      "2024-10-30 17:44:11,057 - INFO - Saved 245000 embeddings to embeddings_batch_245.json\n",
      "2024-10-30 17:44:37,229 - INFO - Saved 246000 embeddings to embeddings_batch_246.json\n",
      "2024-10-30 17:45:02,944 - INFO - Saved 247000 embeddings to embeddings_batch_247.json\n",
      "2024-10-30 17:45:28,409 - INFO - Saved 248000 embeddings to embeddings_batch_248.json\n",
      "2024-10-30 17:45:54,741 - INFO - Saved 249000 embeddings to embeddings_batch_249.json\n",
      "2024-10-30 17:46:21,258 - INFO - Saved 250000 embeddings to embeddings_batch_250.json\n",
      "2024-10-30 17:46:47,142 - INFO - Saved 251000 embeddings to embeddings_batch_251.json\n",
      "2024-10-30 17:47:13,000 - INFO - Saved 252000 embeddings to embeddings_batch_252.json\n",
      "2024-10-30 17:47:42,472 - INFO - Saved 253000 embeddings to embeddings_batch_253.json\n",
      "2024-10-30 17:48:10,767 - INFO - Saved 254000 embeddings to embeddings_batch_254.json\n",
      "2024-10-30 17:48:36,981 - INFO - Saved 255000 embeddings to embeddings_batch_255.json\n",
      "2024-10-30 17:49:04,871 - INFO - Saved 256000 embeddings to embeddings_batch_256.json\n",
      "2024-10-30 17:49:49,055 - INFO - Saved 257000 embeddings to embeddings_batch_257.json\n",
      "2024-10-30 17:50:16,674 - INFO - Saved 258000 embeddings to embeddings_batch_258.json\n",
      "2024-10-30 17:50:43,047 - INFO - Saved 259000 embeddings to embeddings_batch_259.json\n",
      "2024-10-30 17:51:10,436 - INFO - Saved 260000 embeddings to embeddings_batch_260.json\n",
      "2024-10-30 17:51:37,778 - INFO - Saved 261000 embeddings to embeddings_batch_261.json\n",
      "2024-10-30 17:52:04,244 - INFO - Saved 262000 embeddings to embeddings_batch_262.json\n",
      "2024-10-30 17:52:30,483 - INFO - Saved 263000 embeddings to embeddings_batch_263.json\n",
      "2024-10-30 17:52:58,386 - INFO - Saved 264000 embeddings to embeddings_batch_264.json\n",
      "2024-10-30 17:53:27,129 - INFO - Saved 265000 embeddings to embeddings_batch_265.json\n",
      "2024-10-30 17:53:54,824 - INFO - Saved 266000 embeddings to embeddings_batch_266.json\n",
      "2024-10-30 17:54:23,671 - INFO - Saved 267000 embeddings to embeddings_batch_267.json\n",
      "2024-10-30 17:54:50,834 - INFO - Saved 268000 embeddings to embeddings_batch_268.json\n",
      "2024-10-30 17:55:18,822 - INFO - Saved 269000 embeddings to embeddings_batch_269.json\n",
      "2024-10-30 17:55:50,229 - INFO - Saved 270000 embeddings to embeddings_batch_270.json\n",
      "2024-10-30 17:56:17,398 - INFO - Saved 271000 embeddings to embeddings_batch_271.json\n",
      "2024-10-30 17:56:43,545 - INFO - Saved 272000 embeddings to embeddings_batch_272.json\n",
      "2024-10-30 17:57:10,243 - INFO - Saved 273000 embeddings to embeddings_batch_273.json\n",
      "2024-10-30 17:57:36,401 - INFO - Saved 274000 embeddings to embeddings_batch_274.json\n",
      "2024-10-30 17:58:06,175 - INFO - Saved 275000 embeddings to embeddings_batch_275.json\n",
      "2024-10-30 17:58:33,315 - INFO - Saved 276000 embeddings to embeddings_batch_276.json\n",
      "2024-10-30 17:59:00,319 - INFO - Saved 277000 embeddings to embeddings_batch_277.json\n",
      "2024-10-30 17:59:28,683 - INFO - Saved 278000 embeddings to embeddings_batch_278.json\n",
      "2024-10-30 17:59:55,547 - INFO - Saved 279000 embeddings to embeddings_batch_279.json\n",
      "2024-10-30 18:00:22,058 - INFO - Saved 280000 embeddings to embeddings_batch_280.json\n",
      "2024-10-30 18:00:49,280 - INFO - Saved 281000 embeddings to embeddings_batch_281.json\n",
      "2024-10-30 18:01:17,027 - INFO - Saved 282000 embeddings to embeddings_batch_282.json\n",
      "2024-10-30 18:01:44,545 - INFO - Saved 283000 embeddings to embeddings_batch_283.json\n",
      "2024-10-30 18:02:11,486 - INFO - Saved 284000 embeddings to embeddings_batch_284.json\n",
      "2024-10-30 18:02:38,202 - INFO - Saved 285000 embeddings to embeddings_batch_285.json\n",
      "2024-10-30 18:03:04,443 - INFO - Saved 286000 embeddings to embeddings_batch_286.json\n",
      "2024-10-30 18:03:29,748 - INFO - Saved 287000 embeddings to embeddings_batch_287.json\n",
      "2024-10-30 18:03:55,995 - INFO - Saved 288000 embeddings to embeddings_batch_288.json\n",
      "2024-10-30 18:04:22,694 - INFO - Saved 289000 embeddings to embeddings_batch_289.json\n",
      "2024-10-30 18:04:48,688 - INFO - Saved 290000 embeddings to embeddings_batch_290.json\n",
      "2024-10-30 18:05:14,207 - INFO - Saved 291000 embeddings to embeddings_batch_291.json\n",
      "2024-10-30 18:05:39,730 - INFO - Saved 292000 embeddings to embeddings_batch_292.json\n",
      "2024-10-30 18:06:04,500 - INFO - Saved 293000 embeddings to embeddings_batch_293.json\n",
      "2024-10-30 18:06:29,396 - INFO - Saved 294000 embeddings to embeddings_batch_294.json\n",
      "2024-10-30 18:06:55,373 - INFO - Saved 295000 embeddings to embeddings_batch_295.json\n",
      "2024-10-30 18:07:20,169 - INFO - Saved 296000 embeddings to embeddings_batch_296.json\n",
      "2024-10-30 18:07:45,511 - INFO - Saved 297000 embeddings to embeddings_batch_297.json\n",
      "2024-10-30 18:08:10,787 - INFO - Saved 298000 embeddings to embeddings_batch_298.json\n",
      "2024-10-30 18:08:36,247 - INFO - Saved 299000 embeddings to embeddings_batch_299.json\n",
      "2024-10-30 18:09:02,312 - INFO - Saved 300000 embeddings to embeddings_batch_300.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 71\u001b[0m\n\u001b[1;32m     68\u001b[0m     embed_text_and_save(df, model)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 71\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 68\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m df \u001b[38;5;241m=\u001b[39m preprocess_text(df)\n\u001b[1;32m     67\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(model_name)\n\u001b[0;32m---> 68\u001b[0m \u001b[43membed_text_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 39\u001b[0m, in \u001b[0;36membed_text_and_save\u001b[0;34m(df, model)\u001b[0m\n\u001b[1;32m     37\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (idx, row) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(df\u001b[38;5;241m.\u001b[39miterrows()):\n\u001b[0;32m---> 39\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcleaned_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m],  \n\u001b[1;32m     42\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m: embedding\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     44\u001b[0m     })\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# Save every 1000 embeddings to JSON without overwriting\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/clean_testing/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:621\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 621\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    623\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/clean_testing/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:688\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m     module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[1;32m    687\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[0;32m--> 688\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/clean_testing/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/clean_testing/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/clean_testing/lib/python3.9/site-packages/sentence_transformers/models/Transformer.py:350\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m    348\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 350\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    353\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_tokens, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/clean_testing/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/clean_testing/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/clean_testing/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/clean_testing/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/clean_testing/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/clean_testing/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/clean_testing/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/clean_testing/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/clean_testing/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/clean_testing/lib/python3.9/site-packages/transformers/pytorch_utils.py:248\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/clean_testing/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:639\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/clean_testing/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/clean_testing/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/clean_testing/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 539\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/clean_testing/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/clean_testing/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/clean_testing/lib/python3.9/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import logging\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "\n",
    "def remove_emojis(text):\n",
    "    return re.sub(r'[^\\w\\s,]', '', text)\n",
    "\n",
    "def preprocess_text(df):\n",
    "    logging.info(\"Starting text preprocessing\")\n",
    "    start = time.time()\n",
    "    df = df.reset_index().rename(columns={\"index\": \"id\"})  # Create unique IDs\n",
    "    df = df.dropna(subset=['review_text'])\n",
    "    df['cleaned_text'] = df['review_text'].apply(remove_emojis)\n",
    "    df = df[df['cleaned_text'].apply(lambda x: len(x.split()) >= 10)].reset_index(drop=True)\n",
    "    logging.info(f\"Text preprocessing completed in {time.time() - start:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "def embed_text_and_save(df, model):\n",
    "    logging.info(\"Starting embedding\")\n",
    "    start = time.time()\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    logging.info(f\"Total rows to embed: {total_rows}\")\n",
    "\n",
    "    embeddings = []\n",
    "    for i, (idx, row) in enumerate(df.iterrows()):\n",
    "        embedding = model.encode(row['cleaned_text'], show_progress_bar=False)\n",
    "        embeddings.append({\n",
    "            'id': row['id'],  \n",
    "            'text': row['cleaned_text'],\n",
    "            'embedding': embedding.tolist()\n",
    "        })\n",
    "        \n",
    "        # Save every 1000 embeddings to JSON without overwriting\n",
    "        if (i + 1) % 1000 == 0 or (i + 1) == total_rows:\n",
    "            file_name = f\"embeddings_batch_{(i + 1) // 1000}.json\"\n",
    "            with open(file_name, 'a') as f:\n",
    "                json.dump(embeddings, f)\n",
    "                f.write('\\n')  # Newline to avoid JSON parsing errors\n",
    "            logging.info(f\"Saved {i + 1} embeddings to {file_name}\")\n",
    "            embeddings = []  # Clear the batch after saving\n",
    "    \n",
    "    logging.info(f\"Embedding completed in {time.time() - start:.2f} seconds\")\n",
    "\n",
    "def main():\n",
    "    dataset_path = os.getenv(\"DATASET_PATH\")\n",
    "    model_name = os.getenv(\"EMBEDDING_MODEL_PATH\")\n",
    "\n",
    "    logging.info(\"Loading dataset\")\n",
    "    start = time.time()\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    logging.info(f\"Dataset loaded in {time.time() - start:.2f} seconds\")\n",
    "\n",
    "    df = preprocess_text(df)\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embed_text_and_save(df, model)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9cac38-3ee7-40b9-8e70-ab4bec668f87",
   "metadata": {},
   "source": [
    "# 2. Inspects DataFrame after Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e95b1d80-2a8b-4e81-a61e-b802faa6c168",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/muhamadsyukron/opt/anaconda3/envs/clean_testing/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2024-10-31 09:31:19,794 - Dataset loaded in 16.50 seconds\n",
      "2024-10-31 09:31:19,798 - Starting text preprocessing\n",
      "2024-10-31 09:31:36,388 - Text preprocessing completed in 16.59 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import logging\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "\n",
    "def remove_emojis(text):\n",
    "    return re.sub(r'[^\\w\\s,]', '', text)\n",
    "\n",
    "def preprocess_text(df):\n",
    "    logging.info(\"Starting text preprocessing\")\n",
    "    start = time.time()\n",
    "    df = df.reset_index().rename(columns={\"index\": \"id\"})  # Create unique IDs\n",
    "    df = df.dropna(subset=['review_text'])\n",
    "    df['cleaned_text'] = df['review_text'].apply(remove_emojis)\n",
    "    df = df[df['cleaned_text'].apply(lambda x: len(x.split()) >= 10)].reset_index(drop=True)\n",
    "    logging.info(f\"Text preprocessing completed in {time.time() - start:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "\n",
    "dataset_path = os.getenv(\"DATASET_PATH\")\n",
    "start = time.time()\n",
    "df = pd.read_csv(dataset_path)\n",
    "logging.info(f\"Dataset loaded in {time.time() - start:.2f} seconds\")\n",
    "df = preprocess_text(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f087aa6-20ab-4e71-85ad-e8361eef886e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review_id</th>\n",
       "      <th>pseudo_author_id</th>\n",
       "      <th>author_name</th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>review_likes</th>\n",
       "      <th>author_app_version</th>\n",
       "      <th>review_timestamp</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>bfa8876b-470e-4640-83a7-77427f7f37e8</td>\n",
       "      <td>234382942865437071667</td>\n",
       "      <td>A Google user</td>\n",
       "      <td>I enjoy the awesome UI of this app, and it has...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.1.0.91</td>\n",
       "      <td>2014-05-27 14:36:02</td>\n",
       "      <td>I enjoy the awesome UI of this app, and it has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>bbc1bf95-ed36-41a1-8b98-0f2e314caea5</td>\n",
       "      <td>167276875678680630145</td>\n",
       "      <td>A Google user</td>\n",
       "      <td>As a professional Android developer I'm glad t...</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>1.1.0.91</td>\n",
       "      <td>2014-05-27 15:26:48</td>\n",
       "      <td>As a professional Android developer Im glad to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>eac4e85c-2e13-4626-9072-5e190a285cb5</td>\n",
       "      <td>279544562364680964711</td>\n",
       "      <td>A Google user</td>\n",
       "      <td>If I had to put a $ amount on how much I would...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.1.0.91</td>\n",
       "      <td>2014-05-27 15:34:29</td>\n",
       "      <td>If I had to put a  amount on how much I would ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>5483e616-2c00-4c3e-8566-59b32a91b67f</td>\n",
       "      <td>283295985056957279128</td>\n",
       "      <td>A Google user</td>\n",
       "      <td>Easy to search and discover new music and also...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1.1.0.91</td>\n",
       "      <td>2014-05-27 16:42:06</td>\n",
       "      <td>Easy to search and discover new music and also...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>6a9bbc61-75e2-4ce2-a092-62e74cfda8eb</td>\n",
       "      <td>137463903206137863639</td>\n",
       "      <td>A Google user</td>\n",
       "      <td>After updating to latest version I've got to d...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.1.0.112</td>\n",
       "      <td>2014-05-27 18:16:30</td>\n",
       "      <td>After updating to latest version Ive got to do...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Unnamed: 0                             review_id  \\\n",
       "0   1           1  bfa8876b-470e-4640-83a7-77427f7f37e8   \n",
       "1   4           4  bbc1bf95-ed36-41a1-8b98-0f2e314caea5   \n",
       "2   5           5  eac4e85c-2e13-4626-9072-5e190a285cb5   \n",
       "3   8           8  5483e616-2c00-4c3e-8566-59b32a91b67f   \n",
       "4  12          12  6a9bbc61-75e2-4ce2-a092-62e74cfda8eb   \n",
       "\n",
       "        pseudo_author_id    author_name  \\\n",
       "0  234382942865437071667  A Google user   \n",
       "1  167276875678680630145  A Google user   \n",
       "2  279544562364680964711  A Google user   \n",
       "3  283295985056957279128  A Google user   \n",
       "4  137463903206137863639  A Google user   \n",
       "\n",
       "                                         review_text  review_rating  \\\n",
       "0  I enjoy the awesome UI of this app, and it has...              5   \n",
       "1  As a professional Android developer I'm glad t...              5   \n",
       "2  If I had to put a $ amount on how much I would...              5   \n",
       "3  Easy to search and discover new music and also...              5   \n",
       "4  After updating to latest version I've got to d...              1   \n",
       "\n",
       "   review_likes author_app_version     review_timestamp  \\\n",
       "0             4           1.1.0.91  2014-05-27 14:36:02   \n",
       "1            10           1.1.0.91  2014-05-27 15:26:48   \n",
       "2             4           1.1.0.91  2014-05-27 15:34:29   \n",
       "3             2           1.1.0.91  2014-05-27 16:42:06   \n",
       "4             4          1.1.0.112  2014-05-27 18:16:30   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  I enjoy the awesome UI of this app, and it has...  \n",
       "1  As a professional Android developer Im glad to...  \n",
       "2  If I had to put a  amount on how much I would ...  \n",
       "3  Easy to search and discover new music and also...  \n",
       "4  After updating to latest version Ive got to do...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bd9d43-b1aa-4e64-b141-cc6897e6cb4d",
   "metadata": {},
   "source": [
    "# 3. Add Metadata Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f851c1c9-fa9c-4bfe-af1b-267b760ec95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-31 10:07:20,518 - Dataset loaded successfully\n",
      "2024-10-31 10:08:29,321 - Starting to update embedding JSON files with additional fields\n",
      "2024-10-31 10:08:29,336 - Processing file: embeddings_batch_2.json\n",
      "2024-10-31 10:08:31,732 - Updated file saved: /Users/muhamadsyukron/Main Folder/Mac 2023 Files/Mekari/chatbot_project/dataset/embedding_backup/embeddings_batch_2.json\n",
      "2024-10-31 10:08:31,733 - Processing file: embeddings_batch_1.json\n",
      "2024-10-31 10:08:33,963 - Updated file saved: /Users/muhamadsyukron/Main Folder/Mac 2023 Files/Mekari/chatbot_project/dataset/embedding_backup/embeddings_batch_1.json\n",
      "2024-10-31 10:08:33,965 - All embedding JSON files updated successfully in backup directory\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = os.getenv(\"DATASET_PATH\")\n",
    "df = pd.read_csv(dataset_path)\n",
    "logging.info(\"Dataset loaded successfully\")\n",
    "\n",
    "# Rename 'Unnamed: 0' to 'id' to serve as the unique identifier\n",
    "df = df.rename(columns={\"Unnamed: 0\": \"id\"})\n",
    "\n",
    "# Preprocess and extract date components\n",
    "df = df.dropna(subset=['review_text'])\n",
    "df['cleaned_text'] = df['review_text'].apply(lambda text: re.sub(r'[^\\w\\s,]', '', text))\n",
    "df = df[df['cleaned_text'].apply(lambda x: len(x.split()) >= 10)].reset_index(drop=True)\n",
    "df['review_date'] = pd.to_datetime(df['review_timestamp'], errors='coerce')\n",
    "df['year'] = df['review_date'].dt.year\n",
    "df['month'] = df['review_date'].dt.month\n",
    "df['day'] = df['review_date'].dt.day  # Extract day of the month\n",
    "\n",
    "# Directory paths\n",
    "embedding_directory_path = \"/Users/muhamadsyukron/Main Folder/Mac 2023 Files/Mekari/chatbot_project/dataset/embedding\"\n",
    "backup_directory_path = \"/Users/muhamadsyukron/Main Folder/Mac 2023 Files/Mekari/chatbot_project/dataset/embedding_backup\"\n",
    "\n",
    "# Create the backup directory if it doesn't exist\n",
    "os.makedirs(backup_directory_path, exist_ok=True)\n",
    "\n",
    "def add_fields_to_embeddings(source_dir, target_dir, df):\n",
    "    logging.info(\"Starting to update embedding JSON files with additional fields\")\n",
    "\n",
    "    # Loop through each file in the source directory\n",
    "    for file_name in os.listdir(source_dir):\n",
    "        if file_name.startswith(\"embeddings_batch_\") and file_name.endswith(\".json\"):\n",
    "            source_file_path = os.path.join(source_dir, file_name)\n",
    "            target_file_path = os.path.join(target_dir, file_name)\n",
    "            logging.info(f\"Processing file: {file_name}\")\n",
    "\n",
    "            # Load the embedding data from the source JSON file\n",
    "            embeddings = []\n",
    "            with open(source_file_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    embeddings.extend(json.loads(line))\n",
    "\n",
    "            # Add new fields from the DataFrame\n",
    "            for embedding in embeddings:\n",
    "                embedding_id = embedding['id']\n",
    "                matched_row = df[df['id'] == embedding_id]\n",
    "\n",
    "                if not matched_row.empty:\n",
    "                    # Extract relevant fields\n",
    "                    embedding['review_rating'] = int(matched_row['review_rating'].values[0])\n",
    "                    embedding['year'] = int(matched_row['year'].values[0])\n",
    "                    embedding['month'] = int(matched_row['month'].values[0])\n",
    "                    embedding['day'] = int(matched_row['day'].values[0])  # Extract day as an integer\n",
    "\n",
    "            # Save updated embeddings to the target backup file\n",
    "            with open(target_file_path, 'w') as f:\n",
    "                json.dump(embeddings, f)\n",
    "                f.write('\\n')  # Newline to avoid JSON parsing errors\n",
    "            logging.info(f\"Updated file saved: {target_file_path}\")\n",
    "\n",
    "    logging.info(\"All embedding JSON files updated successfully in backup directory\")\n",
    "\n",
    "# Call the function to update embeddings with additional fields, saving to the backup directory\n",
    "add_fields_to_embeddings(embedding_directory_path, backup_directory_path, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d718d2-0fc5-4a0c-b14a-f2834c99ac44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Clear Testing",
   "language": "python",
   "name": "clean_testing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
